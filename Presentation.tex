\documentclass[9pt,hyperref={pdfpagelabels=false},xcolor=table]{beamer}
\usepackage{multicol}

% Contact information: 
%   Jorge M. Cruz-Duarte (jorge.cruz@tec.mx)
%   Nov. 29, 2019

\input{tecbeamer.tex}

\title{Optimizing Gaussian Processes}  
\author[Michael Ciccotosto-Camp]{{\bf Honours Research Project}} 
%\institute{}
\date{
Michael Ciccotosto-Camp - 44302913 \\
}

\begin{document}

\maketitle

\section{Problem Significance}

\begin{frame}
    \frametitle{Problem Setting and Motivation}
    \begin{itemize}
        \item The idea of studying time series prediction came from a research group from the Gatton campus, lead by Dr Potgieter, analysing crop growth from previous seasons to forecast when certain phenological stages will take place in the current harvest.
    \end{itemize}
    \begin{figure}
        \centering
        \includegraphics[scale=0.18]{img/yan_wheat_GPR_plot.png}
    \end{figure}
\end{frame}

\section{Gaussian Processes}

\begin{frame}
    \frametitle{Introduction to Gaussian Processes}
    \begin{itemize}
        \item A Gaussian Process (GP) is a collection of random variables with index set $I$, such that every finite subset of random variables has a joint Gaussian distribution and are completely characterized by a mean function $m : X \to \mathbb{R}$ and a kernel $k : X \times X \to \mathbb{R}$ (in this context, think of the kernel as a function that provides some notion of similarity between points).
    \end{itemize}
    \begin{align*}
        m(\bm{x})           & = \mathbb{E} \left[ f(\bm{x}) \right]                                          \\
        k (\bm{x}, \bm{x'}) & = \mathbb{E} \left[ (f(\bm{x}) - m(\bm{x})) (f(\bm{x'}) - m(\bm{x'})) \right].
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Predictions}
    \begin{itemize}
        \item Using the assumption that our data can be modelled as a Gaussian process, we can write out the new distribution of the observed noisy values along the points at which we wish to test the underlying function as
              \[
                  \begin{bmatrix}
                      \bm{y} \\
                      \bm{y}_{\star}
                  \end{bmatrix}
                  \sim \mathbb{N}
                  \begin{pmatrix}
                      \bm{0}, &
                      {
                              \begin{bmatrix}
                                  \bm{K_{XX}} + \sigma_n^2 \mathbb{I}_{n \times n} & \bm{K_{X_{\star}X}^{\intercal}} \\
                                  \bm{K_{X_{\star}X}}                              & \bm{K_{X_{\star}X_{\star}}}
                              \end{bmatrix}
                          }
                  \end{pmatrix}.
              \]
              (using the notation $\left( \bm{K}_{\bm{W} \bm{W}'} \right)_{i,j} \triangleq k \left( \bm{w}_i , \bm{w}_j' \right)$)
              \pause
        \item The mean and covariance can then be computed as
              \begin{align*}
                  \overline{\bm{y}_{\star}}           & = \bm{K_{X_{\star}X}} \left[ \bm{K_{XX}} + \sigma_n^2 \mathbb{I}_{n \times n} \right]^{-1} \bm{y}                                                         \\
                  \operatorname{cov} (\bm{y}_{\star}) & = \bm{K_{X_{\star}X_{\star}}} - \bm{K_{X_{\star}X}} \left[ \bm{K_{XX}} + \sigma_n^2 \mathbb{I}_{n \times n} \right]^{-1} \bm{K_{X_{\star}X}}^{\intercal}.
              \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Unoptimized GPR}
    {\centering
        \begin{minipage}{.9\linewidth}
            \begin{algorithm}[H]
                \caption{Unoptimized GPR}
                \SetAlgoLined
                \DontPrintSemicolon
                \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

                \Input{Observations $\bm{X}, \bm{y}$ and a test input $\bm{x}_{\star}$.}
                \Output{A prediction $\overline{f_{\star}} $ with its corresponding variance $ \mathbb{V} \left[ f_{\star} \right]$.}
                \BlankLine
                $\bm{L} = \operatorname{cholesky} \left( \bm{K_{XX}} + \sigma_n^2 \mathbb{I}_{n \times n} \right)$\;
                $\bm{\alpha} = \operatorname{lin-solve} \left( \bm{L}^{\intercal} , \operatorname{lin-solve} \left( \bm{L}, \bm{y} \right) \right)$\;
                $\overline{y_{\star}} = \bm{K_{x_{\star} X}} \bm{\alpha}$\;
                $\bm{v} = \operatorname{lin-solve} \left( \bm{L}, \bm{K_{x_{\star} X}} \right)$\;
                $\mathbb{V} \left[ f_{\star} \right] = \bm{K_{x_{\star} x_{\star}}} - \bm{v}^{\intercal} \bm{v}$\;
                \Return{$\overline{f_{\star}} , \mathbb{V} \left[ f_{\star} \right]$}
                \BlankLine
            \end{algorithm}
        \end{minipage}
        \par
    }
\end{frame}

\begin{frame}
    \frametitle{Problems with Unoptimized GPR}
    {\centering
        \begin{minipage}{.9\linewidth}
            \begin{algorithm}[H]
                \caption{Unoptimized GPR}
                \SetAlgoLined
                \DontPrintSemicolon
                \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

                \Input{Observations $\bm{X}, \bm{y}$ and a prediction inputs $\bm{x}_{\star}$.}
                \Output{A prediction $\overline{f_{\star}} $ with its corresponding variance $ \mathbb{V} \left[ f_{\star} \right]$.}
                \BlankLine
                \textcolor{red}{$\bm{L} = \operatorname{cholesky} \left( \bm{K_{XX}} + \sigma_n^2 \mathbb{I}_{n \times n} \right)$}\;
                \textcolor{red}{$\bm{\alpha} = \operatorname{lin-solve} \left( \bm{L}^{\intercal} , \operatorname{lin-solve} \left( \bm{L}, \bm{y} \right) \right)$}\;
                $\overline{f_{\star}} = \bm{K_{x_{\star} X}} \bm{\alpha}$\;
                \textcolor{red}{$\bm{v} = \operatorname{lin-solve} \left( \bm{L}, \bm{K_{x_{\star} X}} \right)$}\;
                $\mathbb{V} \left[ f_{\star} \right] = \bm{K_{x_{\star} x_{\star}}} - \bm{v}^{\intercal} \bm{v}$\;
                \Return{$\overline{f_{\star}} , \mathbb{V} \left[ f_{\star} \right]$}
                \BlankLine
            \end{algorithm}
        \end{minipage}
        \par
    }
    \begin{itemize}
        \item \textcolor{red}{Lines 1,2 and 4} can be incredibly slow as computing $\bm{K_{XX}}$ doing a Cholesky decomposition and performing linear solves scale poorly as the number of inputs, $n$, grows.
    \end{itemize}
\end{frame}

\section{Nystrom}

\begin{frame}
    \frametitle{Nystrom Approximation}
    \begin{itemize}
        \item The Nystrom method we seek a matrix $\bm{Q}\in \mathbb{R}^{n \times k}$ that satisfies $\norm{\bm{A} - \bm{Q} \bm{Q}^{\ast} \bm{A}}_{F} \leq \varepsilon$, where $\bm{A} \in \mathbb{R}^{n \times n}$ is positive semi definite matrix, to form the rank$-k$ approximation
              \begin{align*}
                  \onslide<1->{}
                  \onslide<2->{\bm{A} & \simeq \bm{Q} \bm{Q}^{\ast} \bm{A}                                                                                                                                \\}
                  \onslide<3->{       & \simeq \bm{Q} \left( \bm{Q}^{\ast} \bm{A} \bm{Q} \right) \bm{Q}^{\ast}                                                                                            \\}
                  \onslide<4->{       & = \bm{Q} \left( \bm{Q}^{\ast} \bm{A} \bm{Q} \right) \left( \bm{Q}^{\ast} \bm{A} \bm{Q} \right)^{\dagger} \left( \bm{Q}^{\ast} \bm{A} \bm{Q} \right) \bm{Q}^{\ast} \\}
                  \onslide<5->{       & \simeq \left( \bm{A} \bm{Q} \right) \left( \bm{Q}^{\ast} \bm{A} \bm{Q} \right)^{\dagger} \left( \bm{Q}^{\ast} \bm{A} \right).}
              \end{align*}
              \onslide<6->
              % \item This tells us that $\bm{K_{XX}}$ can be computed as $\bm{K_{XX}} \simeq \bm{K}_{(:,I)} \bm{K}_{(I,I)} \bm{K}_{(:,I)}^{\intercal}$ (up to a rescaling of entries) where $I$ is a randomly sampled subset of column indices.
    \end{itemize}
\end{frame}

\section{Random Fourier Features}

\begin{frame}
    \frametitle{Random Fourier Feature Approximation}
    \begin{itemize}
        \item The RFF technique hinges on Bochners theorem which characterises positive definite functions (namely kernels) and states that any positive definite functions can be represented as
              \[
                  k \left( \bm{x}, \bm{y} \right) = k \left( \bm{x} - \bm{y} \right) = \int_{\mathbb{C}^d} \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) \mu_k \left( d \bm{\omega} \right)
              \]
              where $\mu_k$ is a positive finite measure on the frequencies of $\bm{\omega}$.
              \pause
        \item This integral can then be approximated via the following Monte Carlo estimate
              \begin{align*}
                  \onslide<1->{}
                  \onslide<2->{k \left( \bm{x} - \bm{y} \right)
                                & = \int_{\mathbb{C}^d} \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) p (\bm{\omega}) \; d \bm{\omega}                                                                                                     \\}
                  \onslide<3->{ & = \mathbb{E}_{\bm{\omega} \sim p (\cdot)} \left( \exp \left( i \langle \bm{\omega} , \bm{x} - \bm{y} \rangle \right) \right)                                                                                                   \\}
                  \onslide<4->{ & \simeq \frac{1}{D} \sum_{j=1}^{D} \exp \left( i \langle \bm{\omega}_{j} , \bm{x} - \bm{y} \rangle \right)                                                                                                                      \\}
                  \onslide<5->{ & = \sum_{j=1}^{D} \left( \frac{1}{\sqrt{D}} \exp \left( i \langle \bm{\omega}_{j} , \bm{x} \rangle \right) \right) \overline{\left( \frac{1}{\sqrt{D}} \exp \left( i \langle \bm{\omega}_{j} , \bm{y} \rangle \right) \right) } \\}
                  \onslide<6->{ & = \langle \varphi (\bm{x}) , \varphi (\bm{y}) \rangle_{\mathbb{C}^D}}
              \end{align*}
    \end{itemize}
\end{frame}

\section{Krylov Subspace Methods}

\begin{frame}
    \frametitle{Krylov Subspace Methods}
    \begin{itemize}
        \item $\bm{A} \bm{x^{\star}} = \bm{b}$.
              \pause
        \item $\bm{x^{\star}} \in \bm{x}_0 + \mathcal{K}_{n} \left( \bm{A},\bm{v} \right)$ where  $\mathcal{K}_{k} \left( \bm{A},\bm{v} \right) = \operatorname{l.s} \left\{ \bm{r}_0, \bm{A} \bm{r}_0, \bm{A}^2 \bm{r}_0, \ldots , \bm{A}^{k-1} \bm{r}_0 \right\}$.
              \pause
        \item CG: $\| \bm{x} - \bm{x}^{\star} \|_{\bm{A}}$ is minimized.
              \pause
        \item MINRES: $\norm{\bm{A} \bm {x} - \bm{b}}_2$ is minimized.
    \end{itemize}
\end{frame}

\section{Results}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{img/results/nys-sigma=0.1-k=80.png}
        \caption{3D-Spatial Network dataset using Nystrom}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{img/results/nys-sigma=1.0-k=20.png}
        \caption{Abalone dataset using Nystrom}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{img/results/nys-sigma=1.0-k=80.png}
        \caption{Temperature dataset using Nystrom}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{img/results/rff-sigma=0.1.png}
        \caption{3D-Spatial Network dataset using RFF}
    \end{figure}
\end{frame}

% \begin{frame}
%     \begin{figure}
%         \centering
%         \subfloat[3D-Spatial Network]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/nys-sigma=0.1-k=80.png}
%             \end{adjustbox}
%         }
%         \subfloat[Abalone]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/nys-sigma=1.0-k=20.png}
%             \end{adjustbox}
%         }
%         \subfloat[Temperature]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/nys-sigma=1.0-k=80.png}
%             \end{adjustbox}
%         }
%         \caption{Comparison of Nystrom methods for various datasets.}
%     \end{figure}
%     \begin{figure}
%         \centering
%         \subfloat[3D-Spatial network]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/rff-sigma=0.1.png}
%             \end{adjustbox}
%         }
%         \subfloat[Abalone]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/rff-sigma=10.0.png}
%             \end{adjustbox}
%         }
%         \subfloat[Wine]{
%             \begin{adjustbox}{width=0.31\textwidth}
%                 \includegraphics[scale=1]{img/results/rff-sigma=2.1.png}
%             \end{adjustbox}
%         }
%         \caption{Comparison of RFF methods for various datasets.}
%     \end{figure}
% \end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \subfloat[Frobenius error]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/results/cmp_rel-sigma=0.1-k=80.png}
            \end{adjustbox}
        }
        \subfloat[Infinity error]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/results/cmp_inf-sigma=0.1-k=80.png}
            \end{adjustbox}
        }
        \caption{Comparison between Nystrom and RFF approximations for the 3D-Spatial network data.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item How do Nystrom and RFF methods compare in terms of prediction?
    \end{itemize}
    \pause
    \begin{figure}
        \centering
        \subfloat[Stock Market dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/stock_market/pred_acc/kern_cmp-sigma=10.0.png}
            \end{adjustbox}
        }
        \subfloat[Temperature dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/temperature_data/pred_acc/kern_cmp-sigma=10.0.png}
            \end{adjustbox}
        }
        \caption{Comparison between Nystrom and RFF approximations in GP prediction.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item How do MINRES and CG methods compare in terms of prediction?
    \end{itemize}
    \begin{figure}
        \centering
        \subfloat[Abalone dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/abalone/pred_acc/lin_cmp-sigma=1.0.png}
            \end{adjustbox}
        }
        \subfloat[Quadratic dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/rastrigin/pred_acc/lin_cmp-sigma=2.1.png}
            \end{adjustbox}
        }
        \caption{Comparison between MINRES and CG in GP prediction.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item Using approximation techniques together.
    \end{itemize}
    \begin{figure}
        \centering
        \subfloat[Stock Market dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/3D_spatial_network/pred_acc/dual_cmp_both-sigma=1.0.png}
            \end{adjustbox}
        }
        \subfloat[Abalone dataset]{
            \begin{adjustbox}{width=0.45\textwidth}
                \includegraphics[scale=1]{img/pred/abalone/pred_acc/dual_cmp_both-sigma=1.0.png}
            \end{adjustbox}
        }
        \caption{Comparison between CG and MINRES when paried with RFF.}
    \end{figure}
\end{frame}

\section{Moving Forward}

\begin{frame}
    \frametitle{Moving Forward}
    \begin{itemize}
        \item Write these results up.
        \item Apply our findings to our initial remote sensing task.
        \item Look at multi-output Gaussian Processes for remote sensing.
    \end{itemize}
\end{frame}

\end{document}